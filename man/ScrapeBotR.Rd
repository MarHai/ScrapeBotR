% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ScrapeBotR.R
\docType{package}
\name{ScrapeBotR}
\alias{ScrapeBotR}
\title{ScrapeBotR: Retrieve Data from a ScrapeBot Database}
\description{
ScrapeBotR allows to easily retrieve (large amounts of) data from a \href{https://github.com/MarHai/ScrapeBot}{ScrapeBot} installation.
The package provides easy-to-use functions to read and export instances, recipes, runs, log information, and data.
Thereby, the package plugs neatly into the tidyverse as it makes heavy use of tibbles.
}
\section{Getting Started}{

\itemize{
\item Create a credentials file to connect to your database. You may do so using \code{\link[=write_credentials]{write_credentials()}}.
\item Create a connection object by using \code{\link[=connect]{connect()}}.
\item List the available recipes through \code{\link[=get_recipes]{get_recipes()}} or the available instances through \code{\link[=get_instances]{get_instances()}}.
\item Get information about specific runs through \code{\link[=get_runs]{get_runs()}}.
\item Collect data via \code{\link[=get_run_data]{get_run_data()}}.
\item Bulk-download (and resize) screenshots from S3 through \code{\link[=collect_screenshots_from_s3]{collect_screenshots_from_s3()}}.
\item \code{\link[=disconnect]{disconnect()}} from your database.
}
}

\section{ScrapeBot}{

The ScrapeBot (without "R") is a non-R tool for so-called "agent-based testing" to automatically visit, modify, and scrape a defined set of webpages regularly.
It was built to automate various web-based tasks and keep track of them in a controllable way for academic research, primarily in the realm of computational social science.
}

\references{
Haim, M. (2020). Agent-based testing: An automated approach toward artificial reactions to human behavior.
Journalism Studies, 21(7), 895-911. \url{https://dx.doi.org/10.1080/1461670x.2019.1702892}
}
\seealso{
\url{https://github.com/MarHai/ScrapeBot}
}
